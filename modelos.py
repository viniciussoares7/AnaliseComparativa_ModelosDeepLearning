# C:\TCC\modelos.py - Com Cache/Save na pasta 'Modelos'

import time
import os
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report
from keras import layers
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50, EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import gc

# Importar configura√ß√µes e constantes
from config import (
    STANDARD_IMAGE_SIZE, INCEPTION_IMAGE_SIZE, NUM_CLASSES, MODELS_DIR
)

# ===================================================================
# 1. FUN√á√ïES AUXILIARES
# ===================================================================

def clean_session():
    """ Limpa a sess√£o anterior do Keras e coleta lixo para liberar mem√≥ria. """
    tf.keras.backend.clear_session()
    gc.collect()

def add_classifier_head(base_model, num_classes=NUM_CLASSES, dropout_rate=0.5, custom_inputs=None):
    """ Adiciona o classificador (camadas densas) ao modelo base congelado. """
    
    # Congela o modelo base
    for layer in base_model.layers:
        layer.trainable = False
        
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    
    # Constr√≥i o modelo completo (Input -> Rescaling -> Base Model -> Head)
    if custom_inputs is not None:
        model = Model(inputs=custom_inputs, outputs=predictions, name=base_model.name)
    else:
        model = Model(inputs=base_model.input, outputs=predictions, name=base_model.name)

    return model

def create_model_with_rescaling(base_model_class, input_shape, model_name):
    """ 
    Fun√ß√£o helper para construir modelos de Transfer Learning.
    Cria uma nova inst√¢ncia de Rescaling com nome √∫nico para cada modelo.
    """
    inputs = Input(shape=input_shape)

    # CORRE√á√ÉO: Cria uma NOVA inst√¢ncia de Rescaling com nome √∫nico para este modelo
    rescale_layer_unique = layers.Rescaling(
        scale=1.0/255, 
        name=f"rescaling_{model_name}" # Nome √önico (ex: rescaling_EfficientNetB0)
    )
    x = rescale_layer_unique(inputs)
    
    # Garante que o input_tensor seja o 'x' normalizado
    base_model = base_model_class(weights='imagenet', include_top=False, input_tensor=x)
    return add_classifier_head(base_model, custom_inputs=inputs)

# ===================================================================
# 2. M√âTODOS DE CONSTRU√á√ÉO DE MODELOS (Builders)
# ===================================================================

def build_vgg16_model(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo VGG16 com Transfer Learning. """
    return create_model_with_rescaling(VGG16, input_shape, 'VGG16')

def build_inceptionv3_model(input_shape=INCEPTION_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo InceptionV3 (exige 299x299). """
    return create_model_with_rescaling(InceptionV3, input_shape, 'InceptionV3')

def build_resnet50_model(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo ResNet50 com Transfer Learning. """
    return create_model_with_rescaling(ResNet50, input_shape, 'ResNet50')

def build_efficientnetb0_model(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo EfficientNetB0 com Transfer Learning. """
    return create_model_with_rescaling(EfficientNetB0, input_shape, 'EfficientNetB0')

def build_custom_cnn(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i a Rede Neural Convolucional Personalizada (CNN). """
    model = Sequential(name='Custom_CNN')

    # Adicionar a camada de normaliza√ß√£o primeiro!
    model.add(Input(shape=input_shape))
    # CORRE√á√ÉO: Cria uma NOVA inst√¢ncia de Rescaling com nome √∫nico
    model.add(layers.Rescaling(1./255, name='rescaling_custom_cnn'))
    
    # Bloco 1
    model.add(Conv2D(32, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.25))

    # Bloco 2
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.25))

    # Bloco 3
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.25))

    # Classificador
    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(NUM_CLASSES, activation='softmax'))

    return model

# ===================================================================
# 3. M√âTODOS DE TREINAMENTO, CACHE E AVALIA√á√ÉO
# ===================================================================

def get_callbacks(model_name):
    """ Retorna a lista de callbacks otimizados, salvando pesos na raiz temporariamente. """
    # O ModelCheckpoint salva apenas os pesos (h5), permitindo que a fun√ß√£o principal carregue.
    return [
        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6),
        ModelCheckpoint(f'{model_name}_best_model.h5', monitor='val_loss', save_best_only=True)
    ]

def train_model(model, train_ds, val_ds, class_weights, model_name, epochs_transfer=10, epochs_finetune=15):
    """
    Fun√ß√£o principal para treinar o modelo em duas fases ou treinamento √∫nico (CNN).
    """
    
    callbacks = get_callbacks(model_name)
    total_training_time = 0.0

    # ----------------------------------------------------
    # FASE 1: Transfer Learning (Treinar Camadas Finais)
    # ----------------------------------------------------
    if epochs_transfer > 0 and model_name != 'Custom_CNN':
        print(f"\n--- {model_name}: FASE 1: Transfer Learning (Camadas Congeladas) ---")
        model.compile(
            optimizer=Adam(learning_rate=1e-4),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        start_time_transfer = time.time()
        model.fit(
            train_ds, epochs=epochs_transfer, validation_data=val_ds,
            class_weight=class_weights, callbacks=callbacks, verbose=1
        )
        total_training_time += (time.time() - start_time_transfer)
        
        try:
            model.load_weights(f'{model_name}_best_model.h5')
        except:
            print(f"‚ö†Ô∏è Aviso: N√£o foi poss√≠vel carregar os pesos da Fase 1 para {model_name}. Continuando...")


    # ----------------------------------------------------
    # FASE 2: Fine-Tuning (Descongelar Camadas) ou CNN (Treinamento √önico)
    # ----------------------------------------------------
    
    if model_name != 'Custom_CNN' and epochs_finetune > 0:
        print(f"\n--- {model_name}: FASE 2: Fine-Tuning (Descongelando Camadas) ---")
        
        unfreeze_map = {'VGG16': 16, 'InceptionV3': 249, 'ResNet50': 143, 'EfficientNetB0': 180}
        unfreeze_from = unfreeze_map.get(model_name, len(model.layers))

        # Descongela as camadas do modelo base para Fine-Tuning
        for layer in model.layers[1:]: # Ignora a camada de Rescaling
            layer.trainable = True
        for layer in model.layers[:unfreeze_from]:
            layer.trainable = False
        
        # Recompila√ß√£o com LR menor
        model.compile(
            optimizer=Adam(learning_rate=1e-5), # LR 10x menor para Fine-Tuning
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        start_time_finetune = time.time()
        model.fit(
            train_ds, epochs=epochs_finetune, validation_data=val_ds,
            class_weight=class_weights, callbacks=callbacks, verbose=1
        )
        total_training_time += (time.time() - start_time_finetune)

    elif model_name == 'Custom_CNN':
        # Treinamento √∫nico para CNN customizada
        epochs = epochs_transfer + epochs_finetune
        print(f"\n--- {model_name}: Treinamento √önico (Total de {epochs} √©pocas) ---")
        model.compile(
            optimizer=Adam(learning_rate=1e-4),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        start_time_cnn = time.time()
        model.fit(
            train_ds, epochs=epochs, validation_data=val_ds,
            class_weight=class_weights, callbacks=callbacks, verbose=1
        )
        total_training_time = (time.time() - start_time_cnn)
        
    return model, total_training_time

def _run_training(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params, final_model_path):
    """ Fun√ß√£o auxiliar para construir, treinar e salvar o modelo. """
    
    clean_session() # Garante um ambiente limpo
    
    # 1. Construir o modelo
    model = model_builder_func(**model_params)
    
    # 2. Treinar o modelo
    if model_name == 'Custom_CNN':
         trained_model, total_time = train_model(
            model, train_ds, val_ds, class_weights, model_name, epochs_transfer=0, epochs_finetune=25
        )
    else:
        trained_model, total_time = train_model(
            model, train_ds, val_ds, class_weights, model_name, epochs_transfer=10, epochs_finetune=15
        )

    # 3. Carregar os melhores pesos salvos pelo ModelCheckpoint durante o treino
    best_weights_path = f'{model_name}_best_model.h5'
    try:
        trained_model.load_weights(best_weights_path)
    except Exception as e:
        print(f"‚ö†Ô∏è Aviso: N√£o foi poss√≠vel carregar os melhores pesos do ModelCheckpoint para salvar em {model_name}. Usando o modelo final da FASE 2. Erro: {e}")

    # 4. Salvar o modelo final treinado no caminho de cache (Modelo Completo)
    print(f"üíæ Salvando modelo final '{model_name}' em: {final_model_path}")
    trained_model.save(final_model_path)
    
    # 5. Opcional: Remover o arquivo de checkpoint tempor√°rio
    if os.path.exists(best_weights_path):
        os.remove(best_weights_path)
        
    return trained_model, total_time


def load_or_train_model(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params):
    """
    Tenta carregar o modelo de cache. Se n√£o existir, treina, salva na pasta MODELS_DIR e retorna.
    """
    
    # 1. Definir o caminho completo do modelo no diret√≥rio 'Modelos'
    model_dir = os.path.join(MODELS_DIR, model_name)
    model_path = os.path.join(model_dir, f'{model_name}_final_model.h5')
    
    # Garantir que a pasta de destino exista
    os.makedirs(model_dir, exist_ok=True)
    
    trained_model = None
    total_time = 0.0
    
    # 2. Verificar se o modelo j√° existe (cache)
    if os.path.exists(model_path):
        print(f"‚úÖ Modelo '{model_name}' encontrado em cache. Carregando para avalia√ß√£o...")
        
        try:
            # Carrega o modelo COMPLETO (arquitetura + pesos + otimizador)
            trained_model = tf.keras.models.load_model(model_path)
            # O tempo de treino √© 0.0 se for carregado do cache
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar o modelo em cache '{model_path}': {e}. Iniciando novo treino...")
            # Se falhar, limpa a sess√£o e tenta treinar
            clean_session() 
            trained_model, total_time = _run_training(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params, model_path)
            
    else:
        print(f"üî• Modelo '{model_name}' n√£o encontrado. Iniciando Treinamento...")
        trained_model, total_time = _run_training(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params, model_path)
        
    return trained_model, total_time


def evaluate_model(model, test_ds, model_name, total_training_time,class_names):
    """
    Avalia o modelo treinado, gera m√©tricas e o relat√≥rio de classifica√ß√£o usando tf.data.Dataset.
    Nota: Se o modelo veio do cache, os pesos j√° est√£o nele. Se veio do treino, os melhores pesos 
    (salvos pelo ModelCheckpoint) foram carregados na fun√ß√£o _run_training antes de salvar.
    """
    
    print(f"\nüìä Avaliando {model_name} no Conjunto de Teste...")
    metrics_zeroed = {k: 0 for k in ['Acur√°cia Teste (%)', 'Precision (Weighted Avg)', 'Recall (Weighted Avg)', 'F1-Score (Weighted Avg)', 'Tempo de Treinamento (s)', 'Tamanho do Modelo (MB)']}
    
    # Obt√©m o caminho do modelo salvo (dentro da pasta Modelos)
    model_dir = os.path.join(MODELS_DIR, model_name)
    model_path = os.path.join(model_dir, f'{model_name}_final_model.h5')

    # Se a avalia√ß√£o ocorrer logo ap√≥s o treino, o arquivo de checkpoint tempor√°rio
    # j√° foi removido, mas o modelo final est√° salvo em model_path.
    if not os.path.exists(model_path):
        print(f"‚ùå Erro: Arquivo de modelo final esperado em {model_path} n√£o encontrado.")
        return metrics_zeroed

    # Predi√ß√µes
    predictions = model.predict(test_ds, verbose=1)
    predicted_classes = np.argmax(predictions, axis=1)

    # Obter r√≥tulos reais do test_ds
    true_classes_list = []
    class_labels_dict = class_names
    
    for _, batch_labels in test_ds.unbatch().as_numpy_iterator():
        true_classes_list.append(np.argmax(batch_labels)) 
    
    true_classes = np.array(true_classes_list)
    class_labels = class_labels_dict

    # Truncar o predito se os tamanhos forem diferentes
    if len(predicted_classes) != len(true_classes):
        print("‚ö†Ô∏è Aviso: Os r√≥tulos previstos e reais t√™m tamanhos diferentes. Ajustando para o menor tamanho.")
        min_len = min(len(predicted_classes), len(true_classes))
        predicted_classes = predicted_classes[:min_len]
        true_classes = true_classes[:min_len]


    # Relat√≥rio de Classifica√ß√£o
    report_dict = classification_report(true_classes, predicted_classes, target_names=class_labels, output_dict=True, zero_division=0)

    # Obter o tamanho do modelo salvo em MB (Agora do caminho de cache)
    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)

    print(f"\n‚è±Ô∏è Tempo Total de Treino ({model_name}): {total_training_time:.2f} segundos")
    print("\nRelat√≥rio de classifica√ß√£o (Teste):\n", classification_report(true_classes, predicted_classes, target_names=class_labels, zero_division=0))
    print(f"\nTamanho Final do Modelo ({model_name}): {model_size_mb:.2f} MB")

    # Retornar as m√©tricas consolidadas
    metrics = {
        'Acur√°cia Teste (%)': report_dict['accuracy'] * 100,
        'Precision (Weighted Avg)': report_dict['weighted avg']['precision'],
        'Recall (Weighted Avg)': report_dict['weighted avg']['recall'],
        'F1-Score (Weighted Avg)': report_dict['weighted avg']['f1-score'],
        'Tempo de Treinamento (s)': total_training_time,
        'Tamanho do Modelo (MB)': model_size_mb
    }
    return metrics