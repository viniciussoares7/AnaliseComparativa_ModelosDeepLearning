# C:\TCC\modelos.py - Com Cache/Save na pasta 'Modelos'

import time
import os
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report
from keras import layers
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50, EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.utils import get_file
import gc
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Importar configura√ß√µes e constantes
from config import (
    STANDARD_IMAGE_SIZE, INCEPTION_IMAGE_SIZE, NUM_CLASSES, MODELS_DIR,PLOTS_DIR,CLASS_WEIGHTS,MODEL_CONFIGS
)

# ===================================================================
# 1. FUN√á√ïES AUXILIARES
# ===================================================================

def clean_session():
    """ Limpa a sess√£o anterior do Keras e coleta lixo para liberar mem√≥ria. """
    tf.keras.backend.clear_session()
    gc.collect()

def add_classifier_head(x_features_input, num_classes, dropout_rate=0.5):
    """ 
    Recebe o tensor de FEATURES (sa√≠da da √∫ltima camada conv do modelo base) 
    e anexa a cabe√ßa do classificador (Pooling e Camadas Densas).
    Retorna o tensor de predi√ß√µes.
    """
    
    # 1. Aplica o Pooling no mapa de features
    x = GlobalAveragePooling2D()(x_features_input)
    
    # 2. Camada Densa 1 (Hidden Layer)
    x = Dense(512, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    
    # 3. Camada de Sa√≠da (Predictions)
    predictions = Dense(num_classes, activation='softmax')(x) 
    
    # Agora, a fun√ß√£o retorna APENAS o tensor de sa√≠da.
    return predictions

def create_model_with_rescaling(base_model_class, input_shape, model_name):
    
    inputs = Input(shape=input_shape)
    rescale_layer_unique = layers.Rescaling(scale=1.0/255, name=f"rescaling_{model_name}")
    x_normalized = rescale_layer_unique(inputs) 
    
    # 1. Constr√≥i o modelo base (a espinha dorsal).
    base_model = base_model_class(
        weights=None,
        include_top=False, 
        input_tensor=x_normalized, 
        input_shape=input_shape
    )
    
    # 2. CAPTURA A SA√çDA do modelo base. Este √© o tensor de FEATURES.
    x_features = base_model.output 
    
    # 3. Adiciona a cabe√ßa classificadora usando o tensor de FEATURES.
    predictions = add_classifier_head(x_features, num_classes=NUM_CLASSES)
    
    # 4. Define e retorna o modelo final.
    model = Model(inputs=inputs, outputs=predictions, name=model_name) 
    
    return model

# ===================================================================
# 2. M√âTODOS DE CONSTRU√á√ÉO DE MODELOS (Builders)
# ===================================================================

def build_vgg16_model(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo VGG16 com Transfer Learning. """
    return create_model_with_rescaling(VGG16, input_shape, 'VGG16')

def build_inceptionv3_model(input_shape=INCEPTION_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo InceptionV3 (exige 299x299). """
    return create_model_with_rescaling(InceptionV3, input_shape, 'InceptionV3')

def build_resnet50_model(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo ResNet50 com Transfer Learning. """
    return create_model_with_rescaling(ResNet50, input_shape, 'ResNet50')

def build_efficientnetb0_model(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i o modelo EfficientNetB0 com Transfer Learning. """
    return create_model_with_rescaling(EfficientNetB0, input_shape, 'EfficientNetB0')

def build_custom_cnn(input_shape=STANDARD_IMAGE_SIZE + (3,)):
    """ Constr√≥i a Rede Neural Convolucional Personalizada (CNN) otimizada. """

    model = Sequential(name='Custom_CNN')

    # Adicionar a camada de normaliza√ß√£o primeiro!
    model.add(Input(shape=input_shape))
    model.add(layers.Rescaling(1./255, name='rescaling_custom_cnn'))
    
    # Bloco 1
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))

    # Bloco 2
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))

    # Bloco 3
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    
    # Bloco 4 (NOVO)
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))

    # Classificador
    model.add(Flatten())
    
    # Camada Densa 1
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    
    # Camada Densa 2 (NOVO)
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.3))
    
    # Sa√≠da
    model.add(Dense(NUM_CLASSES, activation='softmax'))

    return model

# ===================================================================
# 3. M√âTODOS DE TREINAMENTO, CACHE E AVALIA√á√ÉO
# ===================================================================

def get_callbacks(model_name,patience=10):
    """ Retorna a lista de callbacks otimizados, salvando APENAS pesos (H5). """
    filepath = f'{model_name}_best_model.weights.h5'

    return [
        EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=int(patience/2), min_lr=1e-6),
        ModelCheckpoint(
            filepath, 
            monitor='val_loss', 
            save_best_only=True,
            save_weights_only=True, 
        )
    ]

def train_model(model, train_ds, val_ds, class_weights, model_name):
    
    start_time = time.time()
    
    # 1. Obter a configura√ß√£o espec√≠fica do modelo
    config = MODEL_CONFIGS.get(model_name, MODEL_CONFIGS['VGG16']) 

    epochs_transfer = config['epochs_transfer']
    epochs_finetune = config['epochs_finetune']
    initial_lr = config['initial_lr']
    patience = config['patience']
    unfreeze_layers_count = config['unfreeze_layers_count']
    
    # Obter os Callbacks com a paci√™ncia espec√≠fica
    callbacks_list = get_callbacks(model_name, patience=patience)

    history = None

    # --- FASE 1: Transfer Learning (Camadas Congeladas) ---
    if epochs_transfer > 0:
        print(f"\n--- {model_name}: FASE 1: Transfer Learning (Camadas Congeladas) ---")
        
        # Congelar todas as camadas do modelo base
        model.trainable = True 
        for layer in model.layers[:-len(model.layers[-1].weights)]:
            layer.trainable = False 

        # Compila√ß√£o da Fase 1 (Learning Rate Inicial)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),
            loss=tf.keras.losses.CategoricalCrossentropy(),
            metrics=['accuracy']
        )
        
        # Treinamento da Fase 1
        history = model.fit(
            train_ds,
            epochs=epochs_transfer,
            validation_data=val_ds,
            callbacks=callbacks_list,
            class_weight=class_weights,
            verbose=1
        )
        
    # --- FASE 2: Fine-Tuning (Descongelando Camadas) ---
    if epochs_finetune > 0:
        print(f"\n--- {model_name}: FASE 2: Fine-Tuning (Descongelando Camadas) ---")
        
        # Descongelar as camadas
        if model_name != 'Custom_CNN':
            

            num_layers = len(model.layers)
            
            # 1. Garante que todas as camadas do modelo base estejam trein√°veis, 
            # e a l√≥gica abaixo define quem fica congelado ou n√£o.
            for layer in model.layers:
                layer.trainable = True

            # 2. Aplica a l√≥gica de congelamento/descongelamento:
            if unfreeze_layers_count > 0:
                # Descongelar N camadas do final (VGG16 padr√£o)
                layers_unfrozen = unfreeze_layers_count
                # As camadas j√° est√£o trainables=True, esta l√≥gica √© mais simples
            
            elif unfreeze_layers_count < 0:
                # Descongelar tudo, EXCETO as N primeiras camadas (ResNet50/EfficientNet)
                layers_to_keep_frozen = abs(unfreeze_layers_count)
                
                # Congela as N primeiras camadas (o "n√∫cleo" da extra√ß√£o de features)
                for layer in model.layers[:layers_to_keep_frozen]:
                    layer.trainable = False
                
                # As camadas restantes (corpo principal do ResNet50) permanecem True
                layers_unfrozen = num_layers - layers_to_keep_frozen
            
            else:
                # Caso unfreeze_layers_count == 0 ou valor inv√°lido
                layers_unfrozen = 0
                for layer in model.layers:
                    layer.trainable = False

            print(f"‚úÖ Camadas descongeladas para Fine-Tuning: {layers_unfrozen} de {num_layers} camadas totais.")
        
        
        #Usa o LR de Fine-Tuning configurado (fine_tune_lr), se existir
        # Se n√£o existir (como no Custom_CNN), usa o padr√£o (initial_lr * 0.1)
        finetune_lr = config.get('fine_tune_lr', initial_lr * 0.1) 
        
        # Compila√ß√£o da Fase 2 (Novo Learning Rate mais baixo)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=finetune_lr),
            loss=tf.keras.losses.CategoricalCrossentropy(),
            metrics=['accuracy']
        )

        # Treinamento da Fase 2
        history_finetune = model.fit(
            train_ds,
            epochs=epochs_finetune,
            validation_data=val_ds,
            callbacks=callbacks_list,
            class_weight=class_weights,
            verbose=1
        )
        
        # Mesclar hist√≥ricos
        if history:
             for key in history_finetune.history.keys():
                 history.history[key].extend(history_finetune.history[key])
        else:
            history = history_finetune
            
    total_time = time.time() - start_time
    print(f"\n‚è±Ô∏è Tempo Total de Treino ({model_name}): {total_time:.2f} segundos")

    return model, total_time, history


def _run_training(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params, final_model_path):
    """ 
    Fun√ß√£o auxiliar para construir, treinar e salvar o modelo.
    Inclui a corre√ß√£o de carregamento manual de pesos para EfficientNetB0.
    """
    
    clean_session() 
    
    # 1. Construir o modelo
    model = model_builder_func(**model_params)

    weights_path = None # Inicializa weights_path
    
    # 1.1. Carregamento manual para EfficientNetB0 e outros com pesos ImageNet
    if model_name in ['EfficientNetB0', 'VGG16', 'InceptionV3', 'ResNet50']:
        print(f"üîß Aplicando corre√ß√£o de canal: Carregando pesos {model_name} ImageNet manualmente...")
        
        # Mapeamento para obter os pesos notop corretos
        WEIGHTS_MAP = {
            'EfficientNetB0': 'https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5',
            'VGG16': 'https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',
            'InceptionV3': 'https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',
            'ResNet50': 'https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'
        }
        
        weights_url = WEIGHTS_MAP.get(model_name)
        weights_filename = os.path.basename(weights_url)
        
        weights_path = get_file(
            weights_filename,
            weights_url,
            cache_subdir='models',
        )
        
        # Carrega os pesos ImageNet no modelo rec√©m-criado 
        try:
            model.load_weights(weights_path, by_name=True, skip_mismatch=True)
            print("‚úÖ Pesos ImageNet carregados com sucesso (manual).")
        except Exception as e:
             print(f"‚ùå Erro ao carregar pesos ImageNet para {model_name}: {e}")

    # 2. Treinar o modelo
    trained_model, total_time, history = train_model(
        model, 
        train_ds, 
        val_ds, 
        class_weights, 
        model_name, 
    )

    # 3. Carregar os melhores pesos salvos pelo ModelCheckpoint durante o treino
    best_weights_path = f'{model_name}_best_model.weights.h5'

    # 4. Salvar o modelo final treinado no caminho de cache
    print(f"üíæ Salvando modelo final '{model_name}' em: {final_model_path}")

    if model_name in ['EfficientNetB0', 'VGG16', 'ResNet50', 'InceptionV3']:
        trained_model.save_weights(final_model_path)
        print(f"     (Apenas pesos salvos para modelos Transfer Learning.)")
    else:
        # Salva o modelo COMPLETO (arquitetura + pesos) para Custom_CNN
        trained_model.save(final_model_path)
    
    # 5. Remover o arquivo de checkpoint tempor√°rio
    if os.path.exists(best_weights_path):
        os.remove(best_weights_path)
        
    return trained_model, total_time, history


def load_or_train_model(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params):
    """
    Tenta carregar o modelo de cache. Se n√£o existir, treina, salva na pasta MODELS_DIR e retorna.
    """
    
    model_dir = os.path.join(MODELS_DIR, model_name)
    model_path = os.path.join(model_dir, f'{model_name}_final_model.h5')
    
    os.makedirs(model_dir, exist_ok=True)
    
    trained_model = None
    total_time = 0.0
    history = None
    
    # 2. Verificar se o modelo j√° existe (cache)
    if os.path.exists(model_path):
        print(f"‚úÖ Modelo '{model_name}' encontrado em cache. Carregando para avalia√ß√£o...")
        
        try:
            if model_name in ['EfficientNetB0', 'VGG16', 'ResNet50', 'InceptionV3']:
                # üö® Carregamento Pontual: Reconstrua e carregue apenas os pesos.
                clean_session() 
                trained_model = model_builder_func(**model_params)
                
                # Tenta carregar os pesos, se falhar o try/except cai no re-treino
                trained_model.load_weights(model_path)
                config = MODEL_CONFIGS.get(model_name, MODEL_CONFIGS['VGG16'])
                finetune_lr = config.get('fine_tune_lr', config['initial_lr'] * 0.1) 
                trained_model.compile(
                    optimizer=tf.keras.optimizers.Adam(learning_rate=finetune_lr),
                    loss=tf.keras.losses.CategoricalCrossentropy(),
                    metrics=['accuracy']
                )

            else:
                # Carrega o modelo COMPLETO (Custom_CNN)
                trained_model = tf.keras.models.load_model(model_path)
            
        except Exception as e:
            # ERRO DE ARQUITETURA/PESOS/SERIALIZA√á√ÉO - For√ßar novo treino
            print(f"‚ùå Erro ao carregar o modelo em cache '{model_path}': {e}. Iniciando novo treino...")
            clean_session() 
            trained_model, total_time, history = _run_training(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params, model_path)
            
    else:
        print(f"üî• Modelo '{model_name}' n√£o encontrado. Iniciando Treinamento...")
        trained_model, total_time, history = _run_training(model_name, model_builder_func, train_ds, val_ds, class_weights, model_params, model_path)
        
    return trained_model, total_time, history


def evaluate_model(model, test_ds, model_name, total_training_time,class_names,history=None):
    """
    Avalia o modelo treinado, gera m√©tricas e o relat√≥rio de classifica√ß√£o usando tf.data.Dataset.
    """
    
    print(f"\nüìä Avaliando {model_name} no Conjunto de Teste...")
    metrics_zeroed = {k: 0 for k in ['Acur√°cia Teste (%)', 'Precision (Weighted Avg)', 'Recall (Weighted Avg)', 'F1-Score (Weighted Avg)', 'Tempo de Treinamento (s)', 'Tamanho do Modelo (MB)']}
    
    model_dir = os.path.join(MODELS_DIR, model_name)
    model_path = os.path.join(model_dir, f'{model_name}_final_model.h5')

    if not os.path.exists(model_path):
        print(f"‚ùå Erro: Arquivo de modelo final esperado em {model_path} n√£o encontrado.")
        return metrics_zeroed

    # Predi√ß√µes
    predictions = model.predict(test_ds, verbose=1)
    predicted_classes = np.argmax(predictions, axis=1)

    # Obter r√≥tulos reais do test_ds
    true_classes_list = []
    
    for _, batch_labels in test_ds.unbatch().as_numpy_iterator():
        true_classes_list.append(np.argmax(batch_labels)) 
    
    true_classes = np.array(true_classes_list)
    class_labels = class_names

    # Truncar o predito se os tamanhos forem diferentes (caso raro)
    if len(predicted_classes) != len(true_classes):
        print("‚ö†Ô∏è Aviso: Os r√≥tulos previstos e reais t√™m tamanhos diferentes. Ajustando para o menor tamanho.")
        min_len = min(len(predicted_classes), len(true_classes))
        predicted_classes = predicted_classes[:min_len]
        true_classes = true_classes[:min_len]

    # Relat√≥rio de Classifica√ß√£o
    report_dict = classification_report(true_classes, predicted_classes, target_names=class_labels, output_dict=True, zero_division=0)

    # Obter o tamanho do modelo salvo em MB
    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)

    print(f"\n‚è±Ô∏è Tempo Total de Treino ({model_name}): {total_training_time:.2f} segundos")
    print("\nRelat√≥rio de classifica√ß√£o (Teste):\n", classification_report(true_classes, predicted_classes, target_names=class_labels, zero_division=0))
    print(f"\nTamanho Final do Modelo ({model_name}): {model_size_mb:.2f} MB")

    # Retornar as m√©tricas consolidadas
    metrics = {
        'Acur√°cia Teste (%)': report_dict['accuracy'] * 100,
        'Precision (Weighted Avg)': report_dict['weighted avg']['precision'],
        'Recall (Weighted Avg)': report_dict['weighted avg']['recall'],
        'F1-Score (Weighted Avg)': report_dict['weighted avg']['f1-score'],
        'Tempo de Treinamento (s)': total_training_time,
        'Tamanho do Modelo (MB)': model_size_mb
    }

    # A. Salvar Matriz de Confus√£o
    plot_and_save_confusion_matrix(true_classes, predicted_classes, class_names, model_name)
    
    # B. Salvar Gr√°ficos de Hist√≥rico (se o modelo foi treinado, n√£o carregado do cache)
    if history is not None:
        plot_and_save_history_metrics(history, model_name)

    return metrics

# ===================================================================
# 4. FUN√á√ïES DE PLOTAGEM E GERA√á√ÉO DE GR√ÅFICOS
# ===================================================================

def plot_and_save_history_metrics(history, model_name):
    """Gera e salva gr√°ficos de Acur√°cia e Perda (Loss) por √©poca."""
    
    plot_dir = os.path.join(PLOTS_DIR, model_name)
    os.makedirs(plot_dir, exist_ok=True)
    
    epochs = range(1, len(history.history['accuracy']) + 1)
    
    # --- Gr√°fico de Acur√°cia ---
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, history.history['accuracy'], label='Acur√°cia de Treinamento')
    plt.plot(epochs, history.history['val_accuracy'], label='Acur√°cia de Valida√ß√£o')
    plt.title(f'Acur√°cia de Treinamento e Valida√ß√£o - {model_name}')
    plt.xlabel('√âpoca')
    plt.ylabel('Acur√°cia')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(plot_dir, f'{model_name}_Acuracia_Historico.png'))
    plt.close()

    # --- Gr√°fico de Perda (Loss) ---
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, history.history['loss'], label='Perda de Treinamento')
    plt.plot(epochs, history.history['val_loss'], label='Perda de Valida√ß√£o')
    plt.title(f'Perda de Treinamento e Valida√ß√£o - {model_name}')
    plt.xlabel('√âpoca')
    plt.ylabel('Perda (Loss)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(plot_dir, f'{model_name}_Perda_Historico.png'))
    plt.close()
    print(f"üñºÔ∏è Gr√°ficos de Hist√≥rico salvos em: {plot_dir}")


def plot_and_save_confusion_matrix(true_classes, predicted_classes, class_names, model_name):
    """Gera e salva a Matriz de Confus√£o."""
    
    plot_dir = os.path.join(PLOTS_DIR, model_name)
    os.makedirs(plot_dir, exist_ok=True)

    cm = confusion_matrix(true_classes, predicted_classes)
    
    plt.figure(figsize=(14, 12))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt="d", 
        cmap="Blues",
        xticklabels=class_names, 
        yticklabels=class_names
    )
    plt.title(f'Matriz de Confus√£o - {model_name}')
    plt.ylabel('R√≥tulo Verdadeiro')
    plt.xlabel('R√≥tulo Predito')
    plt.tight_layout()
    plt.savefig(os.path.join(plot_dir, f'{model_name}_Matriz_Confusao.png'))
    plt.close()
    print(f"üñºÔ∏è Matriz de Confus√£o salva em: {plot_dir}")